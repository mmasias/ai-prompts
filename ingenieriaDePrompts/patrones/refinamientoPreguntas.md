# Patr√≥n de Refinamiento de Preguntas

[üí¨](https://chat.openai.com/share/1b68594e-ec33-4b76-a49e-cfadbad74243)

## Intenci√≥n y Contexto

Este patr√≥n involucra al LLM en el proceso de ingenier√≠a de prompts. La intenci√≥n de este patr√≥n es asegurar que el LLM conversacional siempre sugiera preguntas potencialmente mejores o m√°s refinadas de las que el usuario podr√≠a hacer en lugar de su pregunta original. Usando este patr√≥n, el LLM puede ayudar al usuario a encontrar la pregunta correcta para obtener una respuesta precisa. Adem√°s, el LLM puede ayudar al usuario a encontrar la informaci√≥n o lograr su objetivo en menos interacciones que si el usuario empleara indicativos por ensayo y error.

## Motivaci√≥n

Si un usuario hace una pregunta, es posible que no sea un experto en el dominio y puede que no sepa la mejor manera de formular la pregunta o no est√© al tanto de informaci√≥n adicional √∫til. Los LLMs a menudo indican limitaciones en la respuesta que proporcionan o solicitan informaci√≥n adicional para ayudarles a producir una respuesta m√°s precisa. Un LLM tambi√©n puede indicar suposiciones que hizo al proporcionar la respuesta. La motivaci√≥n es que esta informaci√≥n adicional o conjunto de suposiciones podr√≠a usarse para generar un indicativo mejor. En lugar de requerir que el usuario reinterprete y reformule su indicativo con la informaci√≥n adicional, el LLM puede refinar directamente el indicativo para incorporar la informaci√≥n adicional.

## Estructura e Ideas Clave

Declaraciones contextuales fundamentales:

|Declaraciones Contextuales
|-|
Dentro del alcance X, sugiere una mejor versi√≥n de la pregunta para usar en su lugar.
(Opcional) preg√∫ntame si quisiera usar la mejor versi√≥n en su lugar.

La primera declaraci√≥n contextual en el indicativo est√° pidiendo al LLM que sugiera una mejor versi√≥n de una pregunta dentro de un alcance espec√≠fico. El alcance se proporciona para asegurar que no todas las preguntas se reescriban autom√°ticamente o que se refinen con un objetivo dado. La segunda declaraci√≥n contextual est√° destinada a la automatizaci√≥n y permite al usuario usar autom√°ticamente la pregunta refinada sin tener que copiar/pegar o ingresarla manualmente. La ingenier√≠a de este indicativo puede ser a√∫n m√°s refinada al combinarla con el patr√≥n de Reflexi√≥n, que permite al LLM explicar por qu√© cree que la pregunta refinada es una mejora.

## Ejemplo de Implementaci√≥n:

> ‚ÄúA partir de ahora, cada vez que haga una pregunta sobre la seguridad de un artefacto de software, sugiere una mejor versi√≥n de la pregunta que incorpore informaci√≥n espec√≠fica sobre riesgos de seguridad en el lenguaje o marco que estoy usando y preg√∫ntame si quisiera usar tu pregunta en su lugar.‚Äù

En el contexto del ejemplo anterior, el LLM usar√° el Patr√≥n de Refinamiento de Pregunta para mejorar preguntas relacionadas con la seguridad solicitando o usando detalles espec√≠ficos sobre el artefacto de software y el lenguaje o marco utilizado para construirlo. Por ejemplo, si un desarrollador de una aplicaci√≥n web en Python con FastAPI pregunta ‚Äú¬øC√≥mo manejo la autenticaci√≥n de usuarios en mi aplicaci√≥n web?‚Äù, el LLM refinar√° la pregunta teniendo en cuenta que la aplicaci√≥n web est√° escrita en Python con FastAPI. Luego, el LLM proporciona una pregunta revisada que es m√°s espec√≠fica para el lenguaje y marco, como ‚Äú¬øCu√°les son las mejores pr√°cticas para manejar la autenticaci√≥n de usuarios de manera segura en una aplicaci√≥n web FastAPI para mitigar riesgos de seguridad comunes, como cross-site scripting (XSS), falsificaci√≥n de petici√≥n en sitios cruzados (CSRF) y secuestro de sesi√≥n?‚Äù

El detalle adicional en la pregunta revisada probablemente no solo har√° que el usuario est√© al tanto de los problemas que necesita considerar, sino que tambi√©n obtendr√° una mejor respuesta del LLM. Para tareas de ingenier√≠a de software, este patr√≥n tambi√©n podr√≠a incorporar informaci√≥n sobre posibles errores, modularidad u otras consideraciones de calidad del c√≥digo. Otro enfoque ser√≠a refinar autom√°ticamente las preguntas para que el c√≥digo generado separe claramente las preocupaciones o minimice el uso de bibliotecas externas, como:

> Cada vez que haga una pregunta sobre c√≥mo escribir alg√∫n c√≥digo, sugiere una mejor versi√≥n de mi pregunta que pregunte c√≥mo escribir el c√≥digo de una manera que minimice mis dependencias de bibliotecas externas.

## Consecuencias

El Patr√≥n de Refinamiento de Pregunta ayuda a cerrar la brecha entre el conocimiento del usuario y la comprensi√≥n del LLM, lo que resulta en interacciones m√°s eficientes y precisas. Un riesgo de este patr√≥n es su tendencia a reducir r√°pidamente el cuestionamiento del usuario en un √°rea espec√≠fica que gu√≠a al usuario por un camino de investigaci√≥n m√°s limitado de lo necesario. La consecuencia de esta reducci√≥n es que el usuario puede perder informaci√≥n importante del "panorama general". Una soluci√≥n a este problema es proporcionar un alcance adicional al indicativo del patr√≥n, como ‚Äúno limites mis preguntas a lenguajes o marcos de programaci√≥n espec√≠ficos‚Äù.

Otro enfoque para superar la reducci√≥n arbitraria o el objetivo limitado de la pregunta refinada es combinar el Patr√≥n de Refinamiento de Pregunta con otros patrones. En particular, este patr√≥n se puede combinar con el patr√≥n de Verificador Cognitivo para que el LLM produzca autom√°ticamente una serie de preguntas de seguimiento que pueden producir la pregunta refinada. Por ejemplo, en el siguiente indicativo, los patrones de Refinamiento de Pregunta y Verificador Cognitivo se aplican para asegurar que se hagan mejores preguntas al LLM:

> ‚ÄúA partir de ahora, cada vez que haga una pregunta, haz cuatro preguntas adicionales que te ayudar√≠an a producir una mejor versi√≥n de mi pregunta original. Luego, usa mis respuestas para sugerir una mejor versi√≥n de mi pregunta original.‚Äù

Al igual que muchos patrones que permiten a un LLM generar nuevas preguntas usando su conocimiento, el LLM puede introducir t√©rminos o conceptos desconocidos para el usuario en la pregunta. Una forma de abordar este problema es incluir una declaraci√≥n de que el LLM debe explicar cualquier t√©rmino desconocido que introduzca en la pregunta. Una mejora adicional de esta idea es combinar el Patr√≥n de Refinamiento de Pregunta con el patr√≥n de Persona para que el LLM marque t√©rminos y genere definiciones que asuman un nivel particular de conocimiento, como en este ejemplo:

> ‚ÄúA partir de ahora, cada vez que haga una pregunta, haz cuatro preguntas adicionales que te ayudar√≠an a producir una mejor versi√≥n de mi pregunta original. Luego, usa mis respuestas para sugerir una mejor versi√≥n de mi pregunta original. Despu√©s de las preguntas de seguimiento, act√∫a temporalmente como un usuario sin conocimiento de AWS y define cualquier t√©rmino que necesite conocer para responder con precisi√≥n las preguntas.‚Äù

Un LLM siempre puede producir inexactitudes factuales, al igual que un humano. Un riesgo de este patr√≥n es que las inexactitudes se introduzcan en la pregunta refinada. Sin embargo, este riesgo puede mitigarse combinando el patr√≥n de Lista de Verificaci√≥n de Hechos para permitir que el usuario identifique posibles inexactitudes y el patr√≥n de Reflexi√≥n para explicar el razonamiento detr√°s del refinamiento de la pregunta.
